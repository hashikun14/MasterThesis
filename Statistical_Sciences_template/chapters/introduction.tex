\chapter{Introduction}
\section{Background}
The relationship between music and mathematics has a rich and ancient history, tracing back thousands of years to the time of the Greek philosopher Pythagoras. He is often credited with being the first one to observe the deep connection between numerical ratios and musical harmony. According to a famous story recorded by Nicomachus, Pythagoras once passed by a blacksmith’s shop and noticed that the sounds produced by hammers striking metal differed in pitch. He discovered that the variation in pitch was not random but corresponded to the weight ratios of the hammers. This observation led him to further investigate the properties of sound and string vibrations. He found that when the lengths of strings were in simple integer ratios—such as 2:1, 3:2, or 4:3—they produced harmonious intervals that were naturally pleasing to the ear. From this, Pythagoras concluded that musical harmony is fundamentally rooted in numerical relationships, and that numbers are the essence of musical beauty.\\
\\
Fast forward to the modern era, and the influence of mathematics on music has grown even more profound with the advent of digital technology. Innovations in audio engineering and computer science have revolutionized the way we store, process, and reproduce sound. Complex signals, which are inherently analog and continuous, can now be captured and transformed into digital formats such as WAV, MP3, FLAC, and many others. This transformation is made possible through sophisticated encoding algorithms that break down audio into numerical representations, allowing sound to be stored as sequences of time-based data.\\
\\
In essence, these modern technologies have brought Pythagoras's vision to life in a completely new way. Every piece of music—whether it's a delicate solo piano piece, a powerful orchestral symphony, a deep bassline, or a sharp cymbal crash—can now be represented as a stream of numerical data. Through a process called sampling, recording devices convert continuous sound waves into discrete digital samples, taken at fixed intervals known as the sample rate (commonly 22.05 kHz or 44.1 kHz). These samples capture the amplitude of the sound wave at each point in time, resulting in a digital approximation of the original sound. The final outcome is a sequence of numbers that, when processed and played back, can faithfully reproduce the rich, dynamic experience of live music.\\
\\
The time series form of audio data not only preserves the temporal structure of sound but also provides a solid foundation for mathematical and statistical analysis. By applying techniques like the Fourier transform, we can break down complex audio signals into their frequency components, enabling a deeper understanding of the sound’s characteristics. Machine learning algorithms can be trained on this data to recognize musical styles, classify genres or detect patterns. Furthermore, with the rise of AIGC (Artificial Intelligence Generated Content), we can even create entirely new pieces of music by learning from existing audio datasets. In this way, time series data serves as a vital bridge between the physical world of sound and the digital realm of computation.\\
\\
As far as we know music can be reckoned as various genres. A piece of music from a special kind of genre sharing its unique style, the rhythm, rhyme, tone and etc. are all different. Therefore, as the core element of music classification system, genre has irreplaceable important value in music creation, communication, consumption and research. In essence, the identification of musical genres is not only a simple classification label, but also a structural expression of human musical cognitive system, reflecting the musical aesthetic paradigm formed under the specific historical and cultural background. The importance of music genre is first reflected in its guiding role in music creation practice. Each mature music genre represents a complete music grammar system formed through long-term development, including specific technical specifications such as harmonic progression, rhythm pattern, musical structure, orchestration, etc. For example, the sonata form in classical music, the improvised paragraph in jazz, and the synthesizer timbre design in electronic music are the core features of the corresponding genres. These norms not only provide a referential framework for creators, but also ensure the continuity of music culture through intergenerational inheritance. In the field of music education, genre system forms the basis of systematic teaching. Music colleges usually divide professional directions according to genre. This classification method can help students establish a clear knowledge map and more effectively master music language of a specific style.\\
\\
From the perspective of industrial economy, music genre is the key dimension of market segmentation. Record companies, streaming media platforms and performance markets all rely heavily on genre classification to develop business strategies. According to statistics, Spotify, the world's largest music streaming media service platform, uses more than 5000 genre tags to optimize its recommendation algorithm. This refined classification is directly related to the music consumption market of billions of dollars every year. The commercial value of music genre is also reflected in its close binding with specific consumer groups. Audiences of different ages and cultural backgrounds often show obvious genre preferences. In the digital music era, genre tags have become the basic unit of big data analysis. The platform can not only achieve accurate recommendations, but also predict market trends and guide the creation direction of new works through the genre preference data of users.\\
\\
At the academic research level, music schools provide a systematic analytical framework for musicology research. Musicians can reveal the internal law of music development by comparing the morphological characteristics of different schools. For example, through the comparative study of music schools from the Baroque period to the classical period, scholars have clearly combed out the evolution track of the functional harmony system. In the field of cognitive psychology, music genre research helps scientists understand the mechanism of human auditory perception, and experiments have proved that the audience's brain produces distinctive neural response patterns to concerts of different genres. These research results not only deepen our understanding of the nature of music, but also provide theoretical support for music therapy and other application fields.\\
\\
From the perspective of technological development, the definition and identification of music genres has become a core topic in the field of music information retrieval (MIR). Automatic music genre classification technology is an important benchmark for testing the ability of artificial intelligence to understand music by using signal processing, machine learning and deep learning methods. The International Conference on Music Information Retrieval (ISMIR) holds music genre classification competitions every year to promote the continuous improvement of algorithm performance. It is worth noting that the complexity of music genres also poses unique challenges for AI research, such as how to quantitatively deal with the phenomenon of genre fusion, or how to solve the problem of diversity of genre definitions under different cultural backgrounds. These studies not only have technical value, but also promote the cross disciplinary collision of ideas.\\
\\
The importance of music genres is reflected in many aspects, such as the basic norms of music creation, the classification basis of industrial operation, the living carrier of cultural inheritance, the analysis unit of academic research, the test benchmark of technological development, the observation window of social change, the judgment criteria of legal practice, and the protection objects of cultural heritage. Today, when music is deeply integrated with science and technology, we need to understand music genres from a more dynamic and open perspective. We need to respect the inherent value of traditional genres and accommodate the innovative vitality of emerging genres, so as to give full play to the multidimensional role of music genres in promoting the prosperity and development of human music culture. \\
\\

\section{Literature Review}
Except the time series form of the music, the exploration of feature-based analysis began quite early in the history of audio research. By the significant milestone of collaborative study about a quantitative model of the relationship between pitch and frequency(Stevens S., Volkmann, J. , 1940 \& Stevens S. and J. Volkmann, et al. 1937), the foundation of analyzing the music began to be built. The key contributions are (1) the introduction to the Mel Scale, which represent the nonlinear relationship between physical frequency of sound waves and subjective perceptual pitch of human beings, and transform the nonlinear human perception into a linear scale that is computable and (2) the experiment that used the equal-appearing interval method helping participants rank pure tones of varying frequencies, finding the higher pithch resolution in low-frequency ranges and dismissing perceptual differences at high-frequency. Thus the conclusion was modified (Fant G., 1973) and better accepted by people, used wildly in audio signal processing, music information retrieval and hearing aid design. In, parallel, Shapiro (Shapiro S., 1978) invented a method of detection based on the feature space transformation. Although he aimed to deal with the problem of pictures, but his idea was brought into the domain of audio helping us to get the time-frequency detection for music melody contour. For the greatest revolution work of Mel-Frequency Cepstral Coefficients (MFCC) changed the feature engineering of analyzing audio data. The combination of Mel scale and the cepstral analysis (Davis S. and Mermelstein P., 1980) became the standard of MFCC and the benchmark of feature engineering of audio data. MFCCs have since become a foundational tool in audio processing, serving as a standard for tasks such as speech and music classification, genre detection, speaker identification, and clustering.\\
\\
Before 1998, very little research had been conducted on the classification of music genres. The exploratory work in this area was carried out by Lambrou and his colleagues, who tries statistical approaches to characterizing audio signals. In their pioneering study, they extracted eight statistical features from music signals such as mean, variance, skewness, kurtosis, correlations, entropy, and others to represent the properties of audio data. These features were then used to train and evaluate various classification models, including the Minimum Distance Classifier (MDC), K-Nearest Neighbors (KNN), Least Squares Minimum Distance Classifier (LSMDC), and the Quadrature Classifier (QC), marking an early attempt to computationally categorize music based on signal analysis (Lambrou, T., Kudumakis, P., Speller, R., Sandler, M., \& Linney, A., 1998). Logan examined in some details of MFFCCs and investigate their applicability to modeling the music(Logan B.,2000). Deshpande et al. introduced another approach in 2001 that they utilized particular features extracted from spectrograms and Mel-scaled cepstral coefficients. These features were employed to train some classification models (Deshpande, H., Singh, R.K., \& Nam, U., 2001).\\
\\
Tzanetakis and Cook made a significant contribution to the field for music classification by introducing a framework that used multiple types of audio features to recognize the genres of music (Tzanetakis, G. and Cook, P., 2002). They incorporated timbral texture features using Mel-Frequency Cepstral Coefficients (MFCCs), rhythmic content features extracted via wavelet transforms (WT), and pitch content features, alongside both full-file and real-time processing features. These diverse feature sets were then used to train classification models of Gaussian mixture models (GMMs) and K-Nearest Neighbors (KNN) algorithms. In the same year, Tzanetakis and Cook along Ermolinskyi used pitch histograms through the MIDI files to get the features for classification using a KNN model. Their approach helped establish a solid method for automatic genre classification and highlighted the effectiveness of combining different feature domains. Their work is considered a benchmark in the filed of automatic music classification (AMC), and their work were wildly developed in the following years by other scholars and got great outcomes.\\
\\
Following Tzanetakis and Cook's exemplary work, successors have developed classification of music genre in many different ways. McKay and Fujinaga highlighted the importance of integrating both low-level features, such as spectral characteristics, and high-level features, which are structural characteristics of music (McKay, C., \& Fujinaga, I., 2004). In addition to use classical classification algorithms like K-Nearest Neighbors (KNN), they also experimented with feedforward neural networks (NNs). By training these models on a diverse set of features, they achieved impressive results: over 95\% accuracy for classifying root-level genres and more than 80\% accuracy for leaf-level genres. Pérez-Sancho and his colleagues made an interesting study on music genre classification using MIDI files, which represent musical compositions in a symbolic form rather than as audio wave forms (Pérez-Sancho, C., Iñesta, J. M., \& Calera-Rubio, J., 2005). Unlike many previous approaches that focused on audio signal processing, their research explored the potential of applying text categorization techniques to symbolic music data. They treated sequences of musical events—such as notes, durations, and dynamics—as “musical words” and encoded these musical words to extract features suitable for classification tasks. To perform the classification of genre, they fitted naïve Bayes classifiers and KNN, whose best accuracies are 91.0\% and 93.0\% respectively. Cruz-Alcázar, Vidal-Ruiz, and Pérez-Cortés introduced an approach to music genre classification by drawing parallels between music and natural language processing (NLP), treating musical as symbolic sequences similar to linguistic structures (Cruz-Alcázar P.P., Vidal E., \& Pérez-Cortés J.C., 2003). Their methodology diverged from traditional signal-based feature extraction and instead focused on symbolic and syntactic representations of music. They used a Syntactic Pattern Recognition(SPR) technique called grammatical inference(GI) for pitch representation, duration representation and coded symbol strings of the music, and then trained to models like ECGI (Error-Correcting Grammatical Inference), K-TSI (K-Testable in the Strict Sense Inference) and the well-known N-GRAM, where different K's and N's were tried. The model fitted by N-GRAM performed the best which achieves a very low test error at 3\% for the splitted relative-absolute strategy. Barbedo and Lopes extracted spectral roll-off, loudness, bandwidth and spectral flux as the main features of the audio data. Unlike other researchers who used classification algorithms or machine learning models, Barbedo and Lopes did not train any automated models. Instead, they conducted a pairwise comparison analysis, where two genres were evaluated against each other at a time, which was labor-intensive and time-consuming(Lopes A., 2006). Silla and his colleague introduced a strategy that avoids the difficulty to deal with the high dimension of audio data (C. N. Silla Jr., C. A. A. Kaestner \& A. L. Koerich, 2007). Three segments to represent the whole music are sampled from each music piece: one from the beginning, one from the middle, and one from the end, which are all around 1.153 frames in MP3 files. Then they extract the features from these segments and fitted models of classification on each segment including decision trees, KNN, naïve Bayes, support vector machines (SVM) and multi layer perception neural network (MLP). The final genre prediction was then determined through a majority voting mechanism, where the class predicted by at least two of the three segment classifiers would be selected as the overall output, which is the basic idea of ensemble. Garcia and his colleagues introduced a multi-level approach to music genre classification, focusing on both subsong-level and song-level features (Garcia D., 2012). For subsong-level features, they extracted MFCCs. The core innovation of their work, however, was in the song-level features, which were derived using Hidden Markov Models (HMMs). After extracting these features, Garcia and his team applied SVMs for classification, using specialized kernels that generated by HMM and state-space dynamical metrics. Anan and her colleagues tried a new method for classification named LPBoost, which is based on dissimilarity measures rather than traditional feature vectors (Anan Y., Hatano K., Bannai H. \& Takeda M. ,2011). Their approach was tested against SVMs with various kernels, and it achieved superior results on the test set. Chathuranga extracted the features similar to that of Tzanetakis and Cook's, but his proposed features SVM acted as strong base learner in AdaBoost (Chathuranga D., 2013).\\ 
\\
In the past 10 years, the rapid development of computer hardware technology has brought revolutionary changes to the field of audio analysis and processing. The significant improvement of GPU parallel computing capability and the popularity of large capacity memory provide strong support for complex audio analysis algorithms. This progress has promoted the innovation of traditional audio processing methods, and deep learning technology has gradually become the mainstream of research in recent years. For temporal data processing, recurrent neural network (RNN) and its improved architecture show unique advantages, especially long and short term memory network (LSTM) and gated cyclic unit (GRU), which can effectively capture temporal dependencies in audio signals. At the same time, the automatic music classification technology has gone through the transition from relying on manual design of acoustic features to adopting end-to-end deep learning. Among them, the CRNN architecture combining convolutional neural network (CNN) and LSTM is particularly outstanding, and the classification accuracy on the standard data set has increased 15-20 percentage points compared with the traditional methods. Thus the introduction of Transformer architecture brings a new breakthrough to audio analysis and processing. Its self attention mechanism can more effectively model long-distance dependencies. The innovation of training methods also promotes technological progress. Migration learning and semi-supervised learning effectively solve the problem of high costs for data labeling. In practical applications, technologies such as data enhancement and model compression have improved the practicability of the system, enabling the real-time music classification system to achieve millisecond response on mobile devices. At present, research frontiers are exploring the possibility of multimodal fusion. By combining audio signals with lyrics text, album cover images and other information, the latest multimodal Transformer architecture has achieved very high accuracy in music emotion recognition tasks. With the deepening of research, the evaluation system of music classification has become increasingly perfect. Researchers not only pay attention to traditional indicators, but also pay more attention to the generalization ability and interpretability of models.\\
\\
Unfortunately, the use of personal computers is generally lack of computing power. However, these advanced technologies above must be supported by huge computing power. The audio data is difficult to process because of its high dimension (the sampling rate is usually 22.05kHz or 44.1kHz), which means that even a few minutes of audio files will produce millions of data points, putting high demands on computing resources. At the same time, the method of deep learning has not low algorithm complexity, especially when processing time series data, models like LSTM and Transformer often require billions of floating point operations, which makes the training process extremely sensitive to the number of GPU video memory and computing cores. Although I would like to try these methods, due to the limitations of hardware devices, especially ordinary personal computers are usually only equipped with consumer grade graphics cards and limited memory bandwidth, many results that require large-scale parallel computing are difficult to achieve.\\









