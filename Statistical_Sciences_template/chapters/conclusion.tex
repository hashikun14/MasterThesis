\chapter*{Conclusions}
\addcontentsline{toc}{chapter}{Conclusions}
Mel-frequency cepstral coefficients (MFCC) have shown powerful capabilities in the task of automatic music classification, mainly due to their high simulation of human auditory characteristics and accurate extraction of key features of music signals. MFCC effectively captures the perceptual features related to pitch and timbre in music by converting the spectrum into a nonlinear frequency domain representation based on the Mel scale, and is particularly suitable for processing the non-stationary characteristics of music signals. The pre-emphasis, framing, and windowing steps included in its calculation process can effectively suppress the interference of environmental noise and recording differences, while the cepstral coefficients extracted by discrete cosine transform (DCT) focus on the macroscopic features of the spectrum envelope, which happens to be highly consistent with the key information such as harmonic structure and resonance peak distribution relied on in tasks such as music style and instrument classification. In addition, the low-dimensional characteristics of MFCC significantly reduce the computational complexity, allowing it to be efficiently combined with machine learning models. This experiment shows that in the classification tasks of genres such as classical, hip-hop, mental, and disco, the classification accuracy of MFCC features is relatively high. This feature representation that takes into account both physiological perception principles and mathematical simplicity makes it one of the enduring basic features in the field of music information retrieval.\\
\\
Although the shapelet tree-based algorithm has shown certain effectiveness in specific music classification tasks, its applicability has obvious limitations. The algorithm can achieve good classification results for some music categories (such as classical), but its ability to distinguish other categories (such as disco) is relatively weak. The root cause of this phenomenon may be the complex humanistic attributes of the music genre classification itself. The division of music genres is essentially an artificially constructed classification system, and its formation process is deeply influenced by multiple factors such as historical culture, social changes, and artistic creation practices, rather than simply determined by the physical characteristics of audio signals. From the historical dimension of music development, there is often a complex relationship of integration and evolution between different genres. Many music genres borrow from each other and influence each other during their formation, resulting in the fact that there may not be absolutely clear boundaries between them in terms of audio features. In addition, musicians of different genres often use similar instrument combinations, and the overlap of instrument use further increases the difficulty of classification based on audio features. Specifically speaking of the limitations of MFCC features, it may be difficult to fully capture the essential differences between music genres by relying solely on the MFCC coefficients in the low-frequency band. Although MFCC can effectively characterize the spectral envelope characteristics of audio signals, it mainly reflects the physical properties of sound, while the distinction between music genres often involves higher-level musical elements, such as harmony, rhythmic patterns, and performance techniques. These musical elements may not be fully expressed in the low-dimensional MFCC feature space.\\
\\
Based on the above analysis results, the application of shapelet tree-based algorithm in music genre classification task should not be limited to single-dimensional time series analysis, but should be extended to multi-dimensional time series analysis. This study of this experiment shows that it is difficult to achieve effective classification of certain music categories (such as disco) by relying only on one-dimensional MFCC coefficients in low-frequency bands. This limitation is largely due to the fact that the feature representation of a single dimension cannot fully reflect the complex characteristics of music signals. From the perspective of signal processing, music is a complex acoustic signal, and the information contained in different frequency bands is significantly different: low-frequency components mainly reflect the rhythm and basic harmonic structure of music, mid-frequency bands often contain the timbre characteristics of human voices and main instruments, and high-frequency components may carry information such as detailed texture and spatial sense of music. Therefore, future research should focus on exploring how to use MFCC coefficients in the full frequency domain to build a more effective classification model. Specifically, it is possible to consider processing MFCC coefficients of different frequency bands as interrelated multi-dimensional time series, and to mine more discriminative music features by analyzing the synergistic change pattern of the characteristics of each frequency band. For example, some music genres may exhibit unique energy distribution patterns or dynamic change characteristics in specific frequency bands (such as mid-high frequencies), which may be completely ignored in a single low-frequency analysis. In addition, the correlation between features in different frequency bands may also contain important genre distinguishing information: classical music may show coordinated changes in energy in each frequency band, while electronic music may show prominent periodic fluctuations in specific frequency bands. At the technical implementation level, the multidimensional shapelet tree algorithm needs to consider how to effectively deal with the computational complexity of high-dimensional time series data. Developing a new multidimensional shapelet extraction algorithm, changing the input one-dimensional time series to a multidimensional time series, can simultaneously consider the joint distribution characteristics of multiple frequency band features. Such improvements are expected to significantly enhance the model's ability to distinguish complex music genres.